{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767851fe-662b-4280-90cf-749a30073259",
   "metadata": {},
   "source": [
    "# <b> 0. IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ea665-b5cb-4dcb-b98f-9610785b5d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, InputLayer, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1d6f1-607c-4f63-992a-a9d79feeb6e0",
   "metadata": {},
   "source": [
    "# <b> 1. CREATING PATHS TO DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff0a744-3903-43ef-a883-249b818ea19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#files path in a dictionary\n",
    "paths = dict()\n",
    "# Linux pc\n",
    "paths['main'] = '/home/javi/Desktop/Python/MNIST_OD'\n",
    "paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset')\n",
    "paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "# Windows pc\n",
    "# paths['main'] = os.path.normcase('D:\\Javi\\Python\\MNIST_OD')#.replace('\\\\','/'))\n",
    "# paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset'))\n",
    "# paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "# paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "# paths['test_img'] = os.path.join(paths['test_data'], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dae5cc-a064-452a-85c3-43ab1b392b79",
   "metadata": {},
   "source": [
    "# <b> 2. CREATING THE YOLO LIKE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ad74-842b-4fe5-b2b6-eacb9733cfee",
   "metadata": {},
   "source": [
    "## 2.1 Neuronal network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36d2cb8-717d-439a-a1bc-5d3738d903e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Yolo_Reshape(tf.keras.layers.Layer):\n",
    "    def __init__(self, grid_size=7, no_bnb=2, no_class=10):\n",
    "        super(Yolo_Reshape, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_bnb\n",
    "        self.no_class = no_class\n",
    "\n",
    "    # def get_config(self):\n",
    "    #     config = super().get_config().copy()\n",
    "    #     config.update({\n",
    "    #         'target_shape': self.target_shape\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # grids 7x7\n",
    "        S = [self.grid_size, self.grid_size]\n",
    "        # classes\n",
    "        C = self.no_class\n",
    "        # no of bounding boxes per grid\n",
    "        B = self.no_of_bnb\n",
    "\n",
    "        idx1 = S[0] * S[1] * C\n",
    "        idx2 = idx1 + S[0] * S[1] * B\n",
    "\n",
    "        # class probabilities\n",
    "        class_probs = tf.reshape(inputs[:, :idx1], (tf.shape(inputs)[0],) + tuple([S[0], S[1], C]))\n",
    "        class_probs = tf.nn.softmax(class_probs)\n",
    "\n",
    "        #confidence\n",
    "        confs = tf.reshape(inputs[:, idx1:idx2], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B]))\n",
    "        confs = tf.math.sigmoid(confs)\n",
    "\n",
    "        # boxes\n",
    "        boxes = tf.reshape(inputs[:, idx2:], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B * 4]))\n",
    "        boxes = tf.math.sigmoid(boxes)\n",
    "\n",
    "        outputs = tf.concat([class_probs, confs, boxes], axis=3)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3124a55-c91d-46b6-921c-8793c066104e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"mnist_yolo_3\" (type MNIST_YOLO).\n\nfailed to allocate memory [Op:AddV2]\n\nCall arguments received:\n  â€¢ inputs=tf.Tensor(shape=(5, 144, 144, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 55\u001b[0m\n\u001b[0;32m     49\u001b[0m blocks_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock1\u001b[39m\u001b[38;5;124m'\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m7\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[0;32m     50\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock2\u001b[39m\u001b[38;5;124m'\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[0;32m     51\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock3\u001b[39m\u001b[38;5;124m'\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     52\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb3\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb4\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m256\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m]}}\n\u001b[0;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m MNIST_YOLO(inpshape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m144\u001b[39m,\u001b[38;5;241m144\u001b[39m,\u001b[38;5;241m1\u001b[39m),bocks_info\u001b[38;5;241m=\u001b[39mblocks_dict, grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, no_of_bnb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, no_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m144\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m144\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mpredict(tf\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m144\u001b[39m,\u001b[38;5;241m144\u001b[39m,\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m, in \u001b[0;36mMNIST_YOLO.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(x)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"mnist_yolo_3\" (type MNIST_YOLO).\n\nfailed to allocate memory [Op:AddV2]\n\nCall arguments received:\n  â€¢ inputs=tf.Tensor(shape=(5, 144, 144, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "class MNIST_YOLO(tf.keras.Model):\n",
    "    def __init__(self, inpshape, bocks_info,\n",
    "                 grid_size=7, no_of_bnb=2, no_class=10):\n",
    "        super().__init__()\n",
    "        self.nConvs = 0\n",
    "        self.nBlocks = 0\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.bocks_info = bocks_info\n",
    "        self.inpshape = inpshape\n",
    "        self.layersList = list()\n",
    "        for key, val in self.bocks_info.items():\n",
    "            self.block_CN_gen(val, self.layersList)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2, activation='sigmoid')\n",
    "        # self.reshape = Reshape((self.grid_size, self.grid_size, self.no_class+self.no_of_bnb*5))\n",
    "        self.reshape = Yolo_Reshape(self.grid_size, self.no_of_bnb, self.no_class)\n",
    "\n",
    "    def block_CN_gen(self, dictionary_info, layers_list):\n",
    "        '''generate a block of convolutions\n",
    "        dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "        for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "            layer_name = 'block{}_conv{}'.format(self.nBlocks, self.nConvs)\n",
    "            layers_list.append(Conv2D(filt, kernel_size=ker, strides=stride, padding=pad,\n",
    "                               activation=act, name=layer_name))\n",
    "            self.nConvs += 1\n",
    "        layers_list.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same',\n",
    "                           name='Mpooling_block{}'.format(self.nBlocks)))\n",
    "        self.nBlocks += 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = self.InputLayer\n",
    "        # x = self.A(inputs)#Input(shape=self.intshape)\n",
    "        x = inputs\n",
    "        for layer in self.layersList:\n",
    "            x = layer(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.reshape(x)\n",
    "        return x#Model(inputs,x)\n",
    "    def summary(self):\n",
    "        inp = Input(shape=self.inpshape, name=\"input_layer\")\n",
    "        model = Model(inputs=[inp], outputs=self.call(inp))\n",
    "        model.summary()\n",
    "        del inp, model\n",
    "        # return model.summary()\n",
    "            \n",
    "blocks_dict = {'block1':{'b1':[32, (7,7), (1,1), 'same', 'relu']},\n",
    "               'block2':{'b1':[64, (3,3), (1,1), 'same', 'relu']},\n",
    "               'block3':{'b1':[64, (1,1), (1,1), 'same', 'relu'], 'b2':[128, (3,3), (1,1), 'same', 'relu'],\n",
    "                         'b3':[128, (1,1), (1,1), 'same', 'relu'], 'b4':[256, (3,3), (1,1), 'same', 'relu']}}\n",
    "model = MNIST_YOLO(inpshape = (144,144,1),bocks_info=blocks_dict, grid_size=7, no_of_bnb=2, no_class=10)\n",
    "\n",
    "print(model(tf.zeros([5,144,144,1])).shape)\n",
    "model.summary()\n",
    "print(model.predict(tf.zeros([5,144,144,1])).shape)\n",
    "\n",
    "# inputs = Input(shape=(140,140,1))\n",
    "# x = model(inputs)\n",
    "# # print(x.numpy())\n",
    "# output = Yolo_Reshape(grid_size=7, no_bnb=2, no_class=10)(x)\n",
    "\n",
    "# mymodel = Model(inputs, output)\n",
    "# mymodel.summary()\n",
    "# class MyModel(tf.keras.Model):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "#     self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "\n",
    "#   def call(self, inputs):\n",
    "#     x = self.dense1(inputs)\n",
    "#     return self.dense2(x)\n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f0ade-3238-4cde-900b-c2aab9b1ddd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def block_CN_gen(dictionary_info, Input_value):\n",
    "    nConvs = 0\n",
    "    nBlocks = 0\n",
    "    '''generate a block of convolutions\n",
    "    dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "    x = Input_value\n",
    "    for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "        layer_name = 'block{}_conv{}'.format(nBlocks,nConvs)\n",
    "        x = Conv2D(filt, kernel_size=ker, strides=stride, padding=pad,\n",
    "                   activation=act, name=layer_name)(x)\n",
    "        nConvs += 1\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same',\n",
    "                     name='Mpooling_block{}'.format(nBlocks))(x)\n",
    "    nBlocks += 1\n",
    "    return x\n",
    "A = Input(shape=(140,140,1))\n",
    "blocks_dict = {'b1':[10, (3,3), (1,1), 'same', 'relu'],'b2':[5, (5,5), (1,1), 'same', 'relu'],\n",
    "               'b3':[2, (7,7), (1,1), 'same', 'relu']}\n",
    "B = block_CN_gen(blocks_dict,A)\n",
    "model = Model(A,B)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf0c48-ba37-4573-95ab-3964d133a25e",
   "metadata": {},
   "source": [
    "## 2.2 Creating a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c821d-8b6e-4094-9344-691c8dfb16f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a17831-bf68-441d-883c-f7c82489a2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A = Input(shape=(140,140,1))\n",
    "blocks_dict = {'b1':[10, (3,3), (1,1), 'same', 'relu'],'b2':[5, (5,5), (1,1), 'same', 'relu'],\n",
    "               'b3':[2, (7,7), (1,1), 'same', 'relu']}\n",
    "A = Conv2D(5, kernel_size=(3,3), strides=(1,1), padding='same',\n",
    "                   activation='relu',input_shape=(140,140,1))(tf.zeros([1,140,140,1]))\n",
    "B = Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same',\n",
    "                   activation='relu')(A)\n",
    "model = Model(A,B)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5093b-4d68-4637-a2fa-a2906cd99776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    # super(MyModel, self).__init__(name='')\n",
    "    super().__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    x = self.dense1(inputs)\n",
    "    if training:\n",
    "      x = self.dropout(x, training=training)\n",
    "    return self.dense2(x)\n",
    "\n",
    "model = MyModel()\n",
    "model(tf.zeros([1, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b95387-11a2-4240-9b65-b799a983902b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49946de-1a6a-47e1-86c3-b7f59fe6efc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "    self.A = Dense(32)\n",
    "    self.flat = Flatten()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    # x += input_tensor\n",
    "    x = (x)\n",
    "    x = self.A(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "_ = block(tf.zeros([1, 2, 3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b3bd0-c4cf-4cce-a816-dbc3938e99e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block.summary()\n",
    "block.predict(tf.zeros([1,140,140,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc02ea-4862-4316-b0fa-a2cf17c99b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pascal_voc_to_dict(filepath):\n",
    "#     \"\"\"Function to get all the objects from the annotation XML file.\"\"\"\n",
    "#     with tf.io.gfile.GFile(annon_filepath, \"r\") as f:\n",
    "#         root = xml.etree.ElementTree.parse(f).getroot()\n",
    "\n",
    "#         # Disable pytype to avoid attribute-error due to find returning\n",
    "#         # Optional[Element]\n",
    "#         # pytype: disable=attribute-error\n",
    "#         size = root.find(\"size\")\n",
    "#         width = float(size.find(\"width\").text)\n",
    "#         height = float(size.find(\"height\").text)\n",
    "#         filePath = size.find(\"filename\").text\n",
    "\n",
    "#         for obj in root.findall(\"object\"):\n",
    "#             # Get object's label name.\n",
    "#             label = obj.find(\"name\").text.lower()\n",
    "#             # Get objects' pose name.\n",
    "#             pose = obj.find(\"pose\").text.lower()\n",
    "#             is_truncated = obj.find(\"truncated\").text == \"1\"\n",
    "#             is_difficult = obj.find(\"difficult\").text == \"1\"\n",
    "#             bndbox = obj.find(\"bndbox\")\n",
    "#             xmax = float(bndbox.find(\"xmax\").text)\n",
    "#             xmin = float(bndbox.find(\"xmin\").text)\n",
    "#             ymax = float(bndbox.find(\"ymax\").text)\n",
    "#             ymin = float(bndbox.find(\"ymin\").text)\n",
    "#             yield {\n",
    "#                       \"label\": label,\n",
    "#                       # \"pose\": pose,\n",
    "#                       \"bbox\": tfds.features.BBox(\n",
    "#                           ymin / height, xmin / width, ymax / height, xmax / width\n",
    "#                       ),\n",
    "#                       # \"is_truncated\": is_truncated,\n",
    "#                       # \"is_difficult\": is_difficult,\n",
    "#                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fd1a1-eb97-4f62-babf-888fd1e55d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files = os.listdir(paths['train_data'])\n",
    "# def map_images_from_path(path):\n",
    "#     img = tf.io.read_file(path)\n",
    "#     img = tf.io.decode_jpeg(img, channels=1)\n",
    "#     return img\n",
    "\n",
    "# data = tf.data.Dataset.list_files(paths['train_data']+'/*.jpg')\n",
    "# data = data.map(map_images_from_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b97336-d6cf-4077-945d-dc5556100114",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data.as_numpy_iterator().next()\n",
    "#img = tf.io.read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a6ad9-830e-48d5-9dd7-93cf5640e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727772f-1765-4666-b63c-3cfa81c523e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.take(1):\n",
    "    print(i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e40d7d-301b-4118-b964-726f5d9fe356",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file(file_path)\n",
    "plt.imshow(tf.io.decode_jpeg(img, channels=1).numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44a986-1fad-4c68-8d20-3c8e4d268b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.datasets.features.BBox(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f306d-0858-4929-bf72-031b7413d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fe117-ed39-475b-88e3-6638babce15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNIST_OD",
   "language": "python",
   "name": "mnist_od"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
