{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767851fe-662b-4280-90cf-749a30073259",
   "metadata": {},
   "source": [
    "# <b> 0. IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ea665-b5cb-4dcb-b98f-9610785b5d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "# from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten  # , InputLayer ,Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LeakyReLU  #, GlobalMaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93db11b0-4b34-4959-82a6-98d3ba501564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1d6f1-607c-4f63-992a-a9d79feeb6e0",
   "metadata": {},
   "source": [
    "# <b> 1. CREATING PATHS TO DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff0a744-3903-43ef-a883-249b818ea19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#files path in a dictionary\n",
    "paths = dict()\n",
    "# Linux pc\n",
    "# paths['main'] = '/home/javi/Desktop/Python/MNIST_OD'\n",
    "# paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset')\n",
    "# paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "# paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "# Windows pc\n",
    "paths['main'] = os.path.normcase('D:\\Javi\\Python\\MNIST_OD')#.replace('\\\\','/'))\n",
    "paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset')\n",
    "paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "paths['test_img'] = os.path.join(paths['test_data'], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dae5cc-a064-452a-85c3-43ab1b392b79",
   "metadata": {},
   "source": [
    "# <b> 2. CREATING THE YOLO LIKE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ad74-842b-4fe5-b2b6-eacb9733cfee",
   "metadata": {},
   "source": [
    "## 2.1 Neuronal network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36d2cb8-717d-439a-a1bc-5d3738d903e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Yolo_Reshape(tf.keras.layers.Layer):\n",
    "    def __init__(self, grid_size=7, no_bnb=2, no_class=10):\n",
    "        super(Yolo_Reshape, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_bnb\n",
    "        self.no_class = no_class\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # grids 7x7\n",
    "        S = [self.grid_size, self.grid_size]\n",
    "        # classes\n",
    "        C = self.no_class\n",
    "        # no of bounding boxes per grid\n",
    "        B = self.no_of_bnb\n",
    "\n",
    "        idx1 = S[0] * S[1] * C\n",
    "        idx2 = idx1 + S[0] * S[1] * B\n",
    "\n",
    "        # class probabilities\n",
    "        class_probs = tf.reshape(inputs[:, :idx1], (tf.shape(inputs)[0],) + tuple([S[0], S[1], C]))\n",
    "        class_probs = tf.nn.softmax(class_probs)\n",
    "\n",
    "        # confidence\n",
    "        confs = tf.reshape(inputs[:, idx1:idx2], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B]))\n",
    "        confs = tf.math.sigmoid(confs)\n",
    "\n",
    "        # boxes\n",
    "        boxes = tf.reshape(inputs[:, idx2:], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B * 4]))\n",
    "        boxes = tf.math.sigmoid(boxes)\n",
    "\n",
    "        outputs = tf.concat([class_probs, confs, boxes], axis=3)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3124a55-c91d-46b6-921c-8793c066104e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 144, 144, 1)]     0         \n",
      "                                                                 \n",
      " conv2D_init (Conv2D)        (None, 138, 138, 32)      1600      \n",
      "                                                                 \n",
      " Mpooling_init (MaxPooling2D  (None, 69, 69, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " block0_conv0 (Conv2D)       (None, 69, 69, 128)       36992     \n",
      "                                                                 \n",
      " block0_conv1 (Conv2D)       (None, 69, 69, 64)        8256      \n",
      "                                                                 \n",
      " block0_conv2 (Conv2D)       (None, 69, 69, 64)        36928     \n",
      "                                                                 \n",
      " Mpooling_block0 (MaxPooling  (None, 35, 35, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " block1_conv3 (Conv2D)       (None, 35, 35, 64)        4160      \n",
      "                                                                 \n",
      " block1_conv4 (Conv2D)       (None, 35, 35, 128)       73856     \n",
      "                                                                 \n",
      " block1_conv5 (Conv2D)       (None, 35, 35, 128)       16512     \n",
      "                                                                 \n",
      " block1_conv6 (Conv2D)       (None, 35, 35, 256)       295168    \n",
      "                                                                 \n",
      " Mpooling_block1 (MaxPooling  (None, 18, 18, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " block2_conv7 (Conv2D)       (None, 18, 18, 256)       65792     \n",
      "                                                                 \n",
      " block2_conv8 (Conv2D)       (None, 18, 18, 256)       590080    \n",
      "                                                                 \n",
      " block2_conv9 (Conv2D)       (None, 18, 18, 256)       65792     \n",
      "                                                                 \n",
      " block2_conv10 (Conv2D)      (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " Mpooling_block2 (MaxPooling  (None, 9, 9, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2D_last (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 245)               6146805   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 490)               120540    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 490)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 980)               481180    \n",
      "                                                                 \n",
      " yolo__reshape (Yolo_Reshape  (None, 7, 7, 20)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,483,629\n",
      "Trainable params: 11,483,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 144, 144, 1)]     0         \n",
      "                                                                 \n",
      " mnist_yolo (MNIST_YOLO)     (None, 7, 7, 20)          11483629  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,483,629\n",
      "Trainable params: 11,483,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MNIST_YOLO(tf.keras.Model):\n",
    "    def __init__(self, inpshape, bocks_info,\n",
    "                 grid_size=7, no_of_bnb=2, no_class=10):\n",
    "        super().__init__()\n",
    "        self.nConvs = 0\n",
    "        self.nBlocks = 0\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.bocks_info = bocks_info\n",
    "        self.inpshape = inpshape\n",
    "        self.layersList = list()\n",
    "        self.inplayer = Conv2D(32, kernel_size=7, strides=1,\n",
    "                               # input_shape=(inpshape),\n",
    "                               activation=LeakyReLU(alpha=0.1),\n",
    "                               kernel_regularizer=l2(5e-4),\n",
    "                               name='conv2D_init')\n",
    "        self.maxPoolinplayer = MaxPooling2D(pool_size=2, strides=2, name='Mpooling_init')\n",
    "        for key, val in self.bocks_info.items():\n",
    "            self.block_CN_gen(val, self.layersList)\n",
    "\n",
    "        self.final_conv = Conv2D(512, kernel_size=3, strides=1,\n",
    "                                 activation=LeakyReLU(alpha=0.1),\n",
    "                                 name='conv2D_last')\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2/4)\n",
    "        self.drop = Dropout(0.3)\n",
    "        self.dense_2 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2/2)\n",
    "        self.dense_3 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2,\n",
    "                              activation='sigmoid')\n",
    "        self.reshape = Yolo_Reshape(self.grid_size, self.no_of_bnb, self.no_class)\n",
    "\n",
    "    def block_CN_gen(self, dictionary_info, layers_list):\n",
    "        '''generate a block of convolutions\n",
    "        dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "        for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "            layer_name = 'block{}_conv{}'.format(self.nBlocks, self.nConvs)\n",
    "            layers_list.append(Conv2D(filt, kernel_size=ker, strides=stride,\n",
    "                                      padding=pad,\n",
    "                                      activation=act,\n",
    "                                      kernel_regularizer=l2(5e-4),\n",
    "                                      name=layer_name))\n",
    "            self.nConvs += 1\n",
    "        layers_list.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "                                        padding='same',\n",
    "                                        name='Mpooling_block{}'.format(self.nBlocks)))\n",
    "        self.nBlocks += 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = self.InputLayer\n",
    "        x = self.inplayer(inputs)\n",
    "        x = self.maxPoolinplayer(x)\n",
    "        for layer in self.layersList:\n",
    "            x = layer(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.reshape(x)\n",
    "        return x  # Model(inputs,x)\n",
    "\n",
    "    def summary(self):\n",
    "        inp = Input(shape=self.inpshape, name=\"input_layer\")\n",
    "        model = Model(inputs=[inp], outputs=self.call(inp))\n",
    "        model.summary()\n",
    "        del inp, model\n",
    "        # return model.summary()\n",
    "\n",
    "\n",
    "blocks_dict = {'block1': {'b1': [128, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b2': [64, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b3': [64, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)]},\n",
    "               'block2': {'b1': [64, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b2': [128, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b3': [128, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b4': [256, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)],},\n",
    "               'block3': {'b1': [256, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b2': [256, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b3': [256, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          'b4': [512, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)]}}#,\n",
    "                          # 'b5': [512, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "                          # 'b6': [1024, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)]}}\n",
    "               # 'block4': {'b1': [256, (1, 1), (1, 1), 'same', LeakyReLU(alpha=0.1)],\n",
    "               #            'b2': [256, (3, 3), (1, 1), 'same', LeakyReLU(alpha=0.1)]}}\n",
    "\n",
    "\n",
    "mini_yolo = MNIST_YOLO(inpshape=(144, 144, 1),\n",
    "                       bocks_info=blocks_dict,\n",
    "                       grid_size=7, no_of_bnb=2,\n",
    "                       no_class=10\n",
    "                       )\n",
    "\n",
    "mini_yolo.summary()\n",
    "\n",
    "inputs = Input(shape=(144, 144, 1), name=\"input_layer\")\n",
    "outputs = mini_yolo(inputs)\n",
    "mini_yolo = Model(inputs=inputs, outputs=outputs)\n",
    "mini_yolo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf0c48-ba37-4573-95ab-3964d133a25e",
   "metadata": {},
   "source": [
    "## 2.2 Creating a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa806093-09a7-44e9-9203-37b7dd1ac91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorA = np.arange(1*7*7*15).reshape([1,7,7,15])\n",
    "# tensorA = tf.Variable(tensorA)\n",
    "# tensorB = np.arange(1*7*7*15).reshape([1,7,7,15])*2\n",
    "# tensorB = tf.Variable(tensorB)\n",
    "# # tf.concat([tensorA,tensorA*2,tensorA*3],axis=-1)\n",
    "# # tf.math.reduce_sum(tf.subtract(tensorB,tensorA),axis=-1)\n",
    "# tensorB[:,:,:,0:1]<=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c821d-8b6e-4094-9344-691c8dfb16f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class yolo_loss_function(tf.keras.losses.Loss):\n",
    "    '''\n",
    "    This class is the loss function for yolo_v1, which is divided in\n",
    "    in 4 parts\n",
    "    '''\n",
    "\n",
    "    def __init__(self, grid_size, no_of_bnb, no_class):\n",
    "        '''Initialating que variables depending on the accuracy of the NN'''\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.coord_param = 5.0\n",
    "        self.noobj_param = 0.5\n",
    "\n",
    "    def get_xy_wh_global(self, xy_local_tensor):\n",
    "        '''\n",
    "        Translating the local coordintes (grid) xy,\n",
    "        in global coordinates (picture) -> [0,1]\n",
    "        Args:\n",
    "        -xy_local tensor: tf tensor with local coorinates\n",
    "        return: xy tensor in global coodinates\n",
    "        '''\n",
    "        # shape = xy_local_tensor.shape[0]\n",
    "        x_global = tf.range(0, self.grid_size, delta=1, dtype=tf.float32, name='range')\n",
    "        x_global = tf.expand_dims(x_global, axis=0)\n",
    "        x_global = tf.expand_dims(x_global, axis=0)\n",
    "        x_global = tf.tile(x_global, [1, self.grid_size, 1])\n",
    "        x_global = tf.expand_dims(x_global, axis=-1)\n",
    "        y_global = tf.range(0, self.grid_size, delta=1, dtype=tf.float32, name='range')\n",
    "        y_global = tf.expand_dims(y_global, axis=0)\n",
    "        y_global = tf.expand_dims(y_global, axis=-1)\n",
    "        y_global = tf.tile(y_global, [1, 1, self.grid_size])\n",
    "        y_global = tf.expand_dims(y_global, axis=-1)\n",
    "        xy_global = tf.concat([x_global, y_global], axis=-1)\n",
    "        # print(xy_global.shape)\n",
    "        # xy_global = tf.tile(xy_global, [shape, 1, 1, 1])\n",
    "        xy_global = tf.add(xy_local_tensor, xy_global)\n",
    "        xy_global = tf.multiply(xy_global, 1/self.grid_size)\n",
    "        return xy_global\n",
    "\n",
    "    def xy_minmax(self, xy_tensor, wh_tensor):\n",
    "        '''\n",
    "        Transform bnb xy centered coordinates in\n",
    "        xy_minmax corners of the bnb\n",
    "        Args:\n",
    "        -xy_tensor: tf tensor with xy centered coordinates\n",
    "        -wh_tensor: tf tensor wiht wh\n",
    "        return xy_min, xy_max corners of the bnb'''\n",
    "        xy_min = xy_tensor - wh_tensor/2\n",
    "        xy_max = xy_tensor + wh_tensor/2\n",
    "        return xy_min, xy_max\n",
    "\n",
    "    def iou(self, xy_pred_mins, xy_pred_maxes, xy_true_mins, xy_true_maxes):\n",
    "        intersection_min = tf.math.maximum(xy_pred_mins, xy_true_mins)     # ?*7*7*2\n",
    "        intersection_max = tf.math.minimum(xy_pred_maxes, xy_true_maxes)   # ?*7*7*2\n",
    "        intersection = tf.math.minimum(tf.subtract(intersection_max, intersection_min), 0.0)  # ?*7*7*2\n",
    "        intersection = tf.multiply(intersection[:, :, :, 0], intersection[:, :, :, 1])              # ?*7*7\n",
    "\n",
    "        pred_area = tf.subtract(xy_pred_maxes, xy_pred_mins)                   # ?*7*7*2\n",
    "        pred_area = tf.multiply(pred_area[:, :, :, 0], pred_area[:, :, :, 0])  # ?*7*7\n",
    "        true_area = tf.subtract(xy_true_maxes, xy_true_mins)                   # ?*7*7*2\n",
    "        true_area = tf.multiply(true_area[:, :, :, 0], true_area[:, :, :, 1])  # ?*7*7\n",
    "\n",
    "        union_areas = pred_area + true_area - intersection  # ?*7*7\n",
    "        iou_scores = intersection / union_areas             # ?*7*7\n",
    "        iou_scores = tf.expand_dims(iou_scores, axis=-1)    # ?*7*7*1\n",
    "\n",
    "        return iou_scores\n",
    "\n",
    "    def choose_max(self, list_of_tensor):\n",
    "        for t, tensor in enumerate(list_of_tensor):\n",
    "            tensor_shape = list_of_tensor[0].shape[0]\n",
    "            if t == 0:\n",
    "                init_tensor = tensor           # ?*7*7*1\n",
    "                # this create a mask of 0s with shape tensor\n",
    "                # tf.zeros cant be use\n",
    "                mask = tf.where(tensor!=-10, 0.0, -1.0)  # ?*7*7*1\n",
    "            else:\n",
    "                init_tensor = tf.math.maximum(init_tensor, tensor)  # ?*7*7*1\n",
    "                mask = tf.where(init_tensor == tensor, tf.cast(t, tf.float32), mask)     # ?*7*7*1\n",
    "        # Create a tensor mask for predict\n",
    "        # tensor_shape.append(self.no_of_bnb)\n",
    "        for i in range(len(list_of_tensor)):\n",
    "            if i == 0:\n",
    "                mask_pred = tf.where(mask == i, 1.0, 0.0)\n",
    "            else:\n",
    "                mask_pred = tf.concat([mask_pred,\n",
    "                                       tf.where(mask == i, 1.0, 0.0)], axis=-1)\n",
    "\n",
    "        # Create a tensor mask for bnb\n",
    "        for i in range(len(list_of_tensor)):\n",
    "            if i == 0:\n",
    "                mask_bnb = tf.tile(tf.where(mask == i, 1.0, 0.0), [1, 1, 1, 4])\n",
    "            else:\n",
    "                mask_bnb = tf.concat([mask_bnb,\n",
    "                                      tf.tile(tf.where(mask == i, 1.0, 0.0),\n",
    "                                              [1, 1, 1, 4])], axis=-1)\n",
    "\n",
    "        return mask_pred, mask_bnb\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # True labels\n",
    "        label_class = y_true[:, :, :, :self.no_class]               # ?*7*7*10\n",
    "        label_box = y_true[:, :, :, self.no_class:self.no_class+4]  # ?*7*7*4\n",
    "        response_mask = y_true[:, :, :, -1:]                        # ?*7*7*1\n",
    "        # Predicted labels\n",
    "        pred_class = y_pred[:, :, :, :self.no_class]                              # ?*7*7*10\n",
    "        pred_trust = y_pred[:, :, :, self.no_class:self.no_class+self.no_of_bnb]  # ?*7*7*2\n",
    "        pred_box = y_pred[:, :, :, self.no_class+self.no_of_bnb:]                 # ?*7*7*8\n",
    "        # Creating a xy label for the corners of the bnb\n",
    "        xy_label_box = self.get_xy_wh_global(label_box[:, :, :, :2])              # ?*7*7*2\n",
    "        wh_label_box = label_box[:, :, :, 2:]                                     # ?*7*7*2\n",
    "        xy_min_label_box, xy_max_label_box = self.xy_minmax(xy_label_box, wh_label_box)\n",
    "        xy_pred_box = [self.get_xy_wh_global(pred_box[:, :, :, 4*x:4*x+2]) for x in range(self.no_of_bnb)]  # [?*7*7*2]\n",
    "        wh_pred_box = [pred_box[:, :, :, 4*x+2:4*x+4] for x in range(self.no_of_bnb)]                       # [?*7*7*2]\n",
    "        xy_minmax_pred_box = [list(self.xy_minmax(xy, wh)) for xy, wh in zip(xy_pred_box,wh_pred_box)]      # [[?*7*7*2, ?*7*7*2]]]]\n",
    "\n",
    "        iou_scores = [self.iou(xy_minmax_pred_box[x][0],\n",
    "                               xy_minmax_pred_box[x][1],\n",
    "                               xy_min_label_box,\n",
    "                               xy_max_label_box) for x in range(self.no_of_bnb)]\n",
    "        mask_pred, mask_bnb = self.choose_max(iou_scores)  # ?*7*7*2, ?*7*7*8\n",
    "        # Calculating the losses of the centers of the objects\n",
    "        # coord * SUM(Iobj_i-j((x - xhat)² + (y - yhat)²))\n",
    "        loss_xy = tf.subtract(tf.concat(xy_pred_box, axis=-1),\n",
    "                              tf.tile(xy_label_box, [1, 1, 1, self.no_of_bnb]))  # ?*7*7*4\n",
    "        loss_xy = tf.math.pow(loss_xy, 2)\n",
    "        loss_xy = tf.tile(mask_pred, [1, 1, 1, self.no_of_bnb])*loss_xy          # ?*7*7*4\n",
    "        loss_xy = tf.tile(response_mask, [1, 1, 1, self.no_of_bnb*2])*loss_xy    # ?*7*7*4\n",
    "        loss_xy = tf.reduce_sum(loss_xy)  # float number\n",
    "        # print(loss_xy)\n",
    "        # Calculating the losses of the weith and heigth of the bb\n",
    "        # coord * SUM(Iobj_i-j((w^0.5 - what^0.5)² + (h^0.5 - hhat^0.5)²))\n",
    "        loss_wh = tf.subtract(tf.math.sqrt(tf.concat(wh_pred_box, axis=-1)),   # ?*7*7*4\n",
    "                              tf.math.sqrt(tf.tile(wh_label_box, [1, 1, 1, self.no_of_bnb])))\n",
    "        # print(loss_wh)\n",
    "        loss_wh = tf.math.pow(loss_wh, 2)\n",
    "        loss_wh = tf.tile(mask_pred, [1, 1, 1, self.no_of_bnb])*loss_wh        # ?*7*7*4\n",
    "        loss_wh = tf.tile(response_mask, [1, 1, 1, self.no_of_bnb*2])*loss_wh  # ?*7*7*4\n",
    "        loss_wh = tf.reduce_sum(loss_wh)  # float number\n",
    "        # print(loss_wh)\n",
    "        # Calculating the losses of the confidence in the predictions\n",
    "        # SUM(Iobj_i-j((C - Chat)²)) + no_objt_param*SUM(Inoobj_i-j(h^0.5 - hhat^0.5)²)\n",
    "        # Inoobj_i-j es de oposite of Iobj_i-j\n",
    "        loss_conf = pred_trust-tf.tile(response_mask,\n",
    "                                       [1, 1, 1, self.no_of_bnb])       # ?*7*7*2\n",
    "        loss_conf = mask_pred*tf.math.pow(loss_conf, 2)                 # ?*7*7*2\n",
    "        loss_conf_noobj = tf.tile(1-response_mask,\n",
    "                                  [1, 1, 1, self.no_of_bnb])*loss_conf  # ?*7*7*2\n",
    "        loss_conf = tf.tile(response_mask,\n",
    "                            [1, 1, 1, self.no_of_bnb])*loss_conf        # ?*7*7*2\n",
    "        loss_conf_noobj = tf.reduce_sum(loss_conf_noobj)  # float number\n",
    "        loss_conf = tf.reduce_sum(loss_conf)  # float number\n",
    "        # print(mask_pred)\n",
    "        # print(tf.tile(response_mask,[1, 1, 1, self.no_of_bnb])*mask_pred)\n",
    "        # print(tf.tile(response_mask,[1, 1, 1, self.no_of_bnb]))\n",
    "        # print(loss_conf)\n",
    "        # print(loss_conf_noobj)\n",
    "        # Calculating the losses of the classes\n",
    "        # SUM_everycell(SUM_classes((C - Chat)²))\n",
    "        classes_loss = tf.subtract(label_class, pred_class)       # ?*7*7*10\n",
    "        classes_loss = tf.math.pow(classes_loss, 2)               # ?*7*7*10\n",
    "        classes_loss = tf.math.reduce_sum(classes_loss, axis=-1,\n",
    "                                          keepdims=True)          # ?*7*7*1\n",
    "        classes_loss = tf.multiply(response_mask, classes_loss)   # ?*7*7*1\n",
    "        classes_loss = tf.math.reduce_sum(classes_loss)           # float number\n",
    "        # print(classes_loss)\n",
    "        # Calculating the total loss\n",
    "        total_loss = self.coord_param*(loss_xy + loss_wh) + loss_conf + self.noobj_param*loss_conf_noobj + classes_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b385f-aea4-4b8c-a1cd-7084ee568281",
   "metadata": {},
   "source": [
    "## 2.3 Custom learning rate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a17831-bf68-441d-883c-f7c82489a2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "    Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    # (0, 0.01),\n",
    "    # (75, 0.001),\n",
    "    # (105, 0.0001),\n",
    "    (0, 0.05),\n",
    "    (5, 0.1),\n",
    "    (10, 0.2),\n",
    "    (40, 0.05),\n",
    "    (80, 0.01),\n",
    "    (110, 0.005),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2adb5-37d8-4a84-bd39-15c258106080",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <b> 3. CREATING THE DATASET TO TRAIN THE NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ee19e-d2f3-47d7-8ff8-43aaea9624b2",
   "metadata": {},
   "source": [
    "## 3.1 Pascal Voc reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcc02ea-4862-4316-b0fa-a2cf17c99b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the mapping function for the dataset. The dataset read the\n",
    "# files of the directory and generate a list of paths, it will return\n",
    "# a tupple with (image, data_info)\n",
    "\n",
    "def pascal_voc_to_dict(filepath):\n",
    "    \"\"\"Function to get all the objects from the annotation XML file.\"\"\"\n",
    "    # The next 3 lines must be modify if the dataset or nn change:\n",
    "    grid_size = 7\n",
    "    no_class = 10\n",
    "    no_of_bnb = 2\n",
    "\n",
    "    with tf.io.gfile.GFile(filepath.numpy().decode(), \"r\") as f:\n",
    "        # print(f.read())\n",
    "        root = ET.parse(f).getroot()\n",
    "\n",
    "        size = root.find(\"size\")\n",
    "        image_w = float(size.find(\"width\").text)\n",
    "        image_h = float(size.find(\"height\").text)\n",
    "        filePath = str(root.find(\"path\").text)\n",
    "        img = tf.io.read_file(filePath)\n",
    "        img = tf.io.decode_jpeg(img, channels=1).numpy()/255.0\n",
    "        # Setting up the output tensor\n",
    "        y_true_tensor = np.zeros([grid_size, grid_size, no_class+5])\n",
    "\n",
    "        for obj in root.findall(\"object\"):\n",
    "            # Get object's label name.\n",
    "            label = int(obj.find(\"name\").text)\n",
    "            # Get objects' pose name.\n",
    "            # pose = obj.find(\"pose\").text.lower()\n",
    "            # is_truncated = obj.find(\"truncated\").text == \"1\"\n",
    "            # is_difficult = obj.find(\"difficult\").text == \"1\"\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            xmax = float(bndbox.find(\"xmax\").text)\n",
    "            xmin = float(bndbox.find(\"xmin\").text)\n",
    "            ymax = float(bndbox.find(\"ymax\").text)\n",
    "            ymin = float(bndbox.find(\"ymin\").text)\n",
    "            # Calculating the middle point of the image and\n",
    "            # (h,w) adimensional -> [0,1]\n",
    "            x = (xmin + xmax) / 2 / image_w\n",
    "            y = (ymin + ymax) / 2 / image_h\n",
    "            w = (xmax - xmin) / image_w\n",
    "            h = (ymax - ymin) / image_h\n",
    "            # Calculate the position of the center inside\n",
    "            # of the grid (i,j) -> (x,y)\n",
    "            loc = [grid_size * x, grid_size * y]\n",
    "            loc_i = int(loc[1])\n",
    "            loc_j = int(loc[0])\n",
    "            y = loc[1] - loc_i\n",
    "            x = loc[0] - loc_j\n",
    "\n",
    "            if y_true_tensor[loc_i, loc_j, no_class+4] == 0:\n",
    "                y_true_tensor[loc_i, loc_j, label] = 1\n",
    "                y_true_tensor[loc_i, loc_j, no_class:no_class+4] = [x, y, w, h]\n",
    "                y_true_tensor[loc_i, loc_j, no_class+4] = 1  # response\n",
    "\n",
    "        return img, y_true_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834fd1a1-eb97-4f62-babf-888fd1e55d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "BATCH = 20\n",
    "PREFETCH = 2\n",
    "SHUFFLE = 100\n",
    "\n",
    "train = tf.data.Dataset.list_files(paths['train_data']+'/*.xml')\n",
    "train = train.map(lambda x: tf.py_function(pascal_voc_to_dict,inp=[x],Tout=[tf.float32, tf.float32]))\n",
    "train =train.batch(BATCH).shuffle(SHUFFLE)#.prefetch(PREFETCH)\n",
    "# list(train)[0][1]\n",
    "test = tf.data.Dataset.list_files(paths['test_data']+'/*.xml')\n",
    "test = test.map(lambda x: tf.py_function(pascal_voc_to_dict,inp=[x],Tout=[tf.float32, tf.float32]))\n",
    "test =test.batch(10)#.prefetch(PREFETCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c79318-7ac9-43c9-a31e-85e213fbc0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # data = data.batch(2)\n",
    "# A = train.as_numpy_iterator().next()\n",
    "# # model(A)\n",
    "# A[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba1d3d5-6b2d-4541-870d-f1de4a95e015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# yolo_loss = yolo_loss_function(7, 2, 10)\n",
    "mini_yolo.compile(loss=yolo_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f56e84-e49d-4bc0-a3ee-562d27cf8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = tf.reshape(tf.range(0,2*7*7*15,1,dtype=tf.float32),[2,7,7,15])#/(2*7*7*15-1)\n",
    "# # B = tf.tile(A,[1,1,1,2])\n",
    "# # B[:,:,:,:15]-B[:,:,:,15:]\n",
    "# # print(A)\n",
    "# B = tf.concat([A[:,:,:,0:10],A[:,:,:,-1:],A[:,:,:,-1:],A[:,:,:,10:14],A[:,:,:,10:14]],\n",
    "#               axis=-1)+0.1\n",
    "# # # print(B)\n",
    "# # yolo_loss(A,B)\n",
    "# yolo_loss.call(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51e602-a7de-4243-85e1-b54b3f92aacf",
   "metadata": {},
   "source": [
    "# <b> 4. TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3a72c-6ba1-4b2b-9f1a-886e58da9611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.0500.\n",
      "Epoch 1/135\n",
      "200/200 [==============================] - 208s 816ms/step - loss: 341.0600 - val_loss: 161.2053\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0500.\n",
      "Epoch 2/135\n",
      "200/200 [==============================] - 167s 800ms/step - loss: 310.9175 - val_loss: 154.8412\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0500.\n",
      "Epoch 3/135\n",
      "200/200 [==============================] - 167s 800ms/step - loss: 307.2849 - val_loss: 152.8575\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0500.\n",
      "Epoch 4/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 306.0926 - val_loss: 152.2776\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0500.\n",
      "Epoch 5/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.5466 - val_loss: 151.9547\n",
      "\n",
      "Epoch 00005: Learning rate is 0.1000.\n",
      "Epoch 6/135\n",
      "200/200 [==============================] - 167s 800ms/step - loss: 305.4026 - val_loss: 151.8091\n",
      "\n",
      "Epoch 00006: Learning rate is 0.1000.\n",
      "Epoch 7/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.3323 - val_loss: 151.7688\n",
      "\n",
      "Epoch 00007: Learning rate is 0.1000.\n",
      "Epoch 8/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2438 - val_loss: 151.6216\n",
      "\n",
      "Epoch 00008: Learning rate is 0.1000.\n",
      "Epoch 9/135\n",
      "200/200 [==============================] - 167s 802ms/step - loss: 305.2768 - val_loss: 151.6182\n",
      "\n",
      "Epoch 00009: Learning rate is 0.1000.\n",
      "Epoch 10/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2614 - val_loss: 151.5871\n",
      "\n",
      "Epoch 00010: Learning rate is 0.2000.\n",
      "Epoch 11/135\n",
      "200/200 [==============================] - 167s 802ms/step - loss: 305.3376 - val_loss: 151.5877\n",
      "\n",
      "Epoch 00011: Learning rate is 0.2000.\n",
      "Epoch 12/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2964 - val_loss: 151.6371\n",
      "\n",
      "Epoch 00012: Learning rate is 0.2000.\n",
      "Epoch 13/135\n",
      "200/200 [==============================] - 168s 802ms/step - loss: 305.2635 - val_loss: 151.5864\n",
      "\n",
      "Epoch 00013: Learning rate is 0.2000.\n",
      "Epoch 14/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.1636 - val_loss: 151.5866\n",
      "\n",
      "Epoch 00014: Learning rate is 0.2000.\n",
      "Epoch 15/135\n",
      "200/200 [==============================] - 168s 802ms/step - loss: 305.2177 - val_loss: 151.5869\n",
      "\n",
      "Epoch 00015: Learning rate is 0.2000.\n",
      "Epoch 16/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2133 - val_loss: 151.5886\n",
      "\n",
      "Epoch 00016: Learning rate is 0.2000.\n",
      "Epoch 17/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2302 - val_loss: 151.5899\n",
      "\n",
      "Epoch 00017: Learning rate is 0.2000.\n",
      "Epoch 18/135\n",
      "200/200 [==============================] - 167s 801ms/step - loss: 305.2000 - val_loss: 151.5888\n",
      "\n",
      "Epoch 00018: Learning rate is 0.2000.\n",
      "Epoch 19/135\n",
      "200/200 [==============================] - ETA: 0s - loss: 305.2270"
     ]
    }
   ],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "mini_yolo.fit(x=train,\n",
    "          # steps_per_epoch=2,  # int(len(X_train) // batch_size),\n",
    "          epochs=135,\n",
    "          verbose=1,\n",
    "          workers=-1,\n",
    "          validation_data = test,\n",
    "          # # validation_steps = int(len(X_val) // batch_size),\n",
    "          callbacks=[CustomLearningRateScheduler(lr_schedule)]#,\n",
    "                    #  mcp_save\n",
    "                    # ]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "948f8dba-e759-46a4-a853-72c3c177f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def xywh2minmax(xy, wh):\n",
    "    xy_min = xy - wh / 2\n",
    "    xy_max = xy + wh / 2\n",
    "\n",
    "    return xy_min, xy_max\n",
    "\n",
    "\n",
    "def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    pred_wh = pred_maxes - pred_mins\n",
    "    true_wh = true_maxes - true_mins\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas\n",
    "\n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "def yolo_head(feats):\n",
    "    # Dynamic implementation of conv dims for fully convolutional model.\n",
    "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
    "    # In YOLO the height index is the inner most iteration.\n",
    "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
    "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
    "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
    "\n",
    "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
    "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
    "    conv_width_index = K.tile(\n",
    "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
    "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
    "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
    "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
    "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
    "\n",
    "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
    "\n",
    "    box_xy = (feats[..., :2] + conv_index) / conv_dims\n",
    "    box_wh = feats[..., 2:4]\n",
    "\n",
    "    return box_xy, box_wh\n",
    "\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    label_class = y_true[..., :10]  # ? * 7 * 7 * 20\n",
    "    label_box = y_true[..., 10:14]  # ? * 7 * 7 * 4\n",
    "    response_mask = y_true[..., 14]  # ? * 7 * 7\n",
    "    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n",
    "\n",
    "    predict_class = y_pred[..., :10]  # ? * 7 * 7 * 20\n",
    "    predict_trust = y_pred[..., 10:12]  # ? * 7 * 7 * 2\n",
    "    predict_box = y_pred[..., 12:]  # ? * 7 * 7 * 8\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
    "\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
    "\n",
    "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n",
    "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n",
    "\n",
    "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n",
    "    # print((1 - box_mask * response_mask).shape)\n",
    "    # print((K.square(predict_trust)).shape)\n",
    "\n",
    "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
    "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
    "    confidence_loss = no_object_loss + object_loss\n",
    "    confidence_loss = K.sum(confidence_loss)\n",
    "\n",
    "    class_loss = response_mask * K.square(label_class - predict_class)\n",
    "    class_loss = K.sum(class_loss)\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "\n",
    "    box_mask = K.expand_dims(box_mask)\n",
    "    response_mask = K.expand_dims(response_mask)\n",
    "\n",
    "    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
    "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
    "    box_loss = K.sum(box_loss)\n",
    "    # print(box_loss)\n",
    "    # print(class_loss)\n",
    "    # print(confidence_loss)\n",
    "\n",
    "    loss = confidence_loss + class_loss + box_loss\n",
    "    # loss.shape\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdc72e-2cdf-4e31-baac-1a5b2ae253f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "\n",
    "paths['save'] = os.path.join( paths['main'], 'model')\n",
    "model.save(os.path.join(paths['save'],'model1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de40da-7922-4dd7-b123-40467263fa8f",
   "metadata": {},
   "source": [
    "# <b> 5. DETECTION ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9664d-ec2b-46df-882d-2fb63e882020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, images, grid, bnb, classes, threshold=0.5):\n",
    "    predictions = model(images)\n",
    "    for img in range(predictions.shape[0]):\n",
    "        image_rect = list()\n",
    "        for row in range(predictions.shape[1]):\n",
    "            for col in range(predictions.shape[2]):\n",
    "                for p, pred in enumerate(predictions[img, row, col, classes: classes+bnb]):\n",
    "                    if p == 0:\n",
    "                        prob = pred\n",
    "                        best = p\n",
    "                    elif pred > prob:\n",
    "                        prob = pred\n",
    "                        best = p\n",
    "                    else:\n",
    "                        pass\n",
    "                if prob > threshold:\n",
    "                    center_x = predictions[img, row, col, classes+bnb+best*4]\n",
    "                    center_y = predictions[img, row, col, classes+bnb+best*4+1]\n",
    "                    w = predictions[img, row, col, classes+bnb+best*4+2]\n",
    "                    h = predictions[img, row, col, classes+bnb+best*4+3]\n",
    "                    image_rect.append([center_x, center_y, w, h])\n",
    "                else:\n",
    "                    pass\n",
    "        image = images[img, :, :, 1]\n",
    "        image = tf.reshape(images.shape[1:])\n",
    "        for rect in image_rect:\n",
    "            x_min = rect[0] - rect[2]/2\n",
    "            x_max = rect[0] + rect[2]/2\n",
    "            y_min = rect[1] - rect[3]/2\n",
    "            y_max = rect[1] + rect[3]/2\n",
    "            image = cv.rectangle(image*255, (x_min, y_min),\n",
    "                                 (x_max, y_max), (255, 0, 0), 2)\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6d51b-a1f1-44c3-a94b-43f69ceb5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092aa9f-044e-4dc7-a046-0047dccc7ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNIST_OD",
   "language": "python",
   "name": "mnist_od"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
