{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767851fe-662b-4280-90cf-749a30073259",
   "metadata": {},
   "source": [
    "# <b> 0. IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6ea665-b5cb-4dcb-b98f-9610785b5d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten  # , InputLayer ,Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LeakyReLU  #, GlobalMaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93db11b0-4b34-4959-82a6-98d3ba501564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1d6f1-607c-4f63-992a-a9d79feeb6e0",
   "metadata": {},
   "source": [
    "# <b> 1. CREATING PATHS TO DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff0a744-3903-43ef-a883-249b818ea19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#files path in a dictionary\n",
    "paths = dict()\n",
    "# Linux pc\n",
    "# paths['main'] = '/home/javi/Desktop/Python/MNIST_OD'\n",
    "# paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset')\n",
    "# paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "# paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "# Windows pc\n",
    "paths['main'] = os.path.normcase('D:\\Javi\\Python\\MNIST_OD')#.replace('\\\\','/'))\n",
    "paths['dataset'] = os.path.join(paths['main'], 'MNIST_dataset')\n",
    "paths['train_data'] = os.path.join(paths['dataset'], 'train')\n",
    "paths['test_data'] = os.path.join(paths['dataset'], 'test')\n",
    "paths['test_img'] = os.path.join(paths['test_data'], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dae5cc-a064-452a-85c3-43ab1b392b79",
   "metadata": {},
   "source": [
    "# <b> 2. CREATING THE YOLO LIKE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666ad74-842b-4fe5-b2b6-eacb9733cfee",
   "metadata": {},
   "source": [
    "## 2.1 Neuronal network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36d2cb8-717d-439a-a1bc-5d3738d903e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Yolo_Reshape(tf.keras.layers.Layer):\n",
    "    def __init__(self, grid_size=7, no_bnb=2, no_class=10):\n",
    "        super(Yolo_Reshape, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_bnb\n",
    "        self.no_class = no_class\n",
    "\n",
    "    # def get_config(self):\n",
    "    #     config = super().get_config().copy()\n",
    "    #     config.update({\n",
    "    #         'target_shape': self.target_shape\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # grids 7x7\n",
    "        S = [self.grid_size, self.grid_size]\n",
    "        # classes\n",
    "        C = self.no_class\n",
    "        # no of bounding boxes per grid\n",
    "        B = self.no_of_bnb\n",
    "\n",
    "        idx1 = S[0] * S[1] * C\n",
    "        idx2 = idx1 + S[0] * S[1] * B\n",
    "\n",
    "        # class probabilities\n",
    "        class_probs = tf.reshape(inputs[:, :idx1], (tf.shape(inputs)[0],) + tuple([S[0], S[1], C]))\n",
    "        class_probs = tf.nn.softmax(class_probs)\n",
    "\n",
    "        #confidence\n",
    "        confs = tf.reshape(inputs[:, idx1:idx2], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B]))\n",
    "        confs = tf.math.sigmoid(confs)\n",
    "\n",
    "        # boxes\n",
    "        boxes = tf.reshape(inputs[:, idx2:], (tf.shape(inputs)[0],) + tuple([S[0], S[1], B * 4]))\n",
    "        boxes = tf.math.sigmoid(boxes)\n",
    "\n",
    "        outputs = tf.concat([class_probs, confs, boxes], axis=3)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3124a55-c91d-46b6-921c-8793c066104e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNIST_YOLO(tf.keras.Model):\n",
    "    def __init__(self, inpshape, bocks_info,\n",
    "                 grid_size=7, no_of_bnb=2, no_class=10):\n",
    "        super().__init__()\n",
    "        self.nConvs = 0\n",
    "        self.nBlocks = 0\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.bocks_info = bocks_info\n",
    "        self.inpshape = inpshape\n",
    "        self.layersList = list()\n",
    "        for key, val in self.bocks_info.items():\n",
    "            self.block_CN_gen(val, self.layersList)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2/4)\n",
    "        self.drop =  Dropout(0.3)\n",
    "        self.dense_2 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2, activation='sigmoid')\n",
    "        # self.reshape = Reshape((self.grid_size, self.grid_size, self.no_class+self.no_of_bnb*5))\n",
    "        self.final_conv = Conv2D(256, kernel_size=3, strides=1, activation='relu', name='conv2D_last')\n",
    "        self.reshape = Yolo_Reshape(self.grid_size, self.no_of_bnb, self.no_class)\n",
    "\n",
    "    def block_CN_gen(self, dictionary_info, layers_list):\n",
    "        '''generate a block of convolutions\n",
    "        dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "        for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "            layer_name = 'block{}_conv{}'.format(self.nBlocks, self.nConvs)\n",
    "            layers_list.append(Conv2D(filt, kernel_size=ker, strides=stride, padding=pad,\n",
    "                               activation=act, name=layer_name))\n",
    "            self.nConvs += 1\n",
    "        layers_list.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same',\n",
    "                           name='Mpooling_block{}'.format(self.nBlocks)))\n",
    "        self.nBlocks += 1\n",
    "\n",
    "    def call(self, x):  #inputs):\n",
    "        # inputs = self.InputLayer\n",
    "        # x = self.A(inputs)#Input(shape=self.intshape)\n",
    "        # x = inputs\n",
    "        for layer in self.layersList:\n",
    "            x = layer(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.reshape(x)\n",
    "        return x  # Model(inputs,x)\n",
    "\n",
    "    def summary(self):\n",
    "        inp = Input(shape=self.inpshape, name=\"input_layer\")\n",
    "        model = Model(inputs=[inp], outputs=self.call(inp))\n",
    "        # model.summary()\n",
    "        # del inp, model\n",
    "        return model.summary()\n",
    "\n",
    "blocks_dict = {'block1':{'b1':[32, (7,7), (1,1), 'same', 'relu']},\n",
    "               'block2':{'b1':[64, (3,3), (1,1), 'same', 'relu']},\n",
    "               'block3':{'b1':[64, (1,1), (1,1), 'same', 'relu'], 'b2':[128, (3,3), (1,1), 'same', 'relu'],\n",
    "                         'b3':[128, (1,1), (1,1), 'same', 'relu'], 'b4':[256, (3,3), (1,1), 'same', 'relu']},\n",
    "               'block4':{'b1':[256, (1,1), (1,1), 'same', 'relu'], 'b2':[256, (3,3), (1,1), 'same', 'relu']}}\n",
    "model = MNIST_YOLO(inpshape=(144, 144, 1),\n",
    "                   bocks_info=blocks_dict,\n",
    "                   grid_size=7, no_of_bnb=2,\n",
    "                   no_class=10,\n",
    "                   # inputs=Input(shape=(144, 144, 1), name=\"input_layer\"),\n",
    "                   # outputs=Yolo_Reshape(7, 2, 10)\n",
    "                  )\n",
    "\n",
    "print(model(tf.zeros([5,144,144,1])).shape)\n",
    "model.summary()\n",
    "# # print(model.predict(tf.zeros([5,144,144,1])).shape)\n",
    "\n",
    "# inputs = Input(shape=(140,140,1))\n",
    "# x = model(inputs)\n",
    "# # print(x.numpy())\n",
    "# output = Yolo_Reshape(grid_size=7, no_bnb=2, no_class=10)(x)\n",
    "\n",
    "# mymodel = Model(inputs, output)\n",
    "# mymodel.summary()\n",
    "# class MyModel(tf.keras.Model):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "#     self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "\n",
    "#   def call(self, inputs):\n",
    "#     x = self.dense1(inputs)\n",
    "#     return self.dense2(x)\n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88fd3f5-6025-4f59-9a40-e5cd271a158c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 144, 144, 1)]     0         \n",
      "                                                                 \n",
      " mnist_yolo_2 (MNIST_YOLO)   (None, 7, 7, 20)          4970349   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,970,349\n",
      "Trainable params: 4,970,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MNIST_YOLO(tf.keras.Model):\n",
    "    def __init__(self, inpshape, bocks_info,\n",
    "                 grid_size=7, no_of_bnb=2, no_class=10, *args, **kwargs):\n",
    "        super().__init__(self,  *args, **kwargs)  # self, *args, **kwargs)\n",
    "        self.nConvs = 0\n",
    "        self.nBlocks = 0\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.bocks_info = bocks_info\n",
    "        self.inpshape = inpshape\n",
    "        self.layersList = list()\n",
    "        # self.inputlayer = Input(shape=self.inpshape, name=\"input_layer\")\n",
    "        for key, val in self.bocks_info.items():\n",
    "            self.block_CN_gen(val, self.layersList)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2/4)\n",
    "        self.drop =  Dropout(0.3)\n",
    "        self.dense_2 = Dense((self.no_class+self.no_of_bnb*5)*self.grid_size**2, activation='sigmoid')\n",
    "        # self.reshape = Reshape((self.grid_size, self.grid_size, self.no_class+self.no_of_bnb*5))\n",
    "        self.final_conv = Conv2D(256, kernel_size=3, strides=1, activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4), name='conv2D_last')\n",
    "        self.reshape = Yolo_Reshape(self.grid_size, self.no_of_bnb, self.no_class)\n",
    "\n",
    "    def block_CN_gen(self, dictionary_info, layers_list):\n",
    "        '''generate a block of convolutions\n",
    "        dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "        for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "            layer_name = 'block{}_conv{}'.format(self.nBlocks, self.nConvs)\n",
    "            layers_list.append(Conv2D(filt, kernel_size=ker, strides=stride, padding=pad,\n",
    "                               activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4), name=layer_name))\n",
    "            self.nConvs += 1\n",
    "        layers_list.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same',\n",
    "                           name='Mpooling_block{}'.format(self.nBlocks)))\n",
    "        self.nBlocks += 1\n",
    "    \n",
    "#     def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'grid_size': self.grid_size,\n",
    "#             'no_of_bnb': self.no_of_bnb,\n",
    "#             'no_class': self.no_class\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "    def call(self, x):  #inputs):\n",
    "        # inputs = self.InputLayer\n",
    "        # x = self.A(inputs)#Input(shape=self.intshape)\n",
    "        # x = inputs\n",
    "        for layer in self.layersList:\n",
    "            x = layer(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.reshape(x)\n",
    "        return x  # Model(self.inputlayer,x)\n",
    "\n",
    "    # def summary(self):\n",
    "    #     inp = Input(shape=self.inpshape, name=\"input_layer\")\n",
    "    #     model = Model(inputs=[inp], outputs=self.call(inp))\n",
    "    #     # model.summary()\n",
    "    #     # del inp, model\n",
    "    #     return model.summary()\n",
    "\n",
    "blocks_dict = {'block1':{'b1':[32, (7,7), (1,1), 'same', 'lrelu']},\n",
    "               'block2':{'b1':[64, (3,3), (1,1), 'same', 'lrelu']},\n",
    "               'block3':{'b1':[64, (1,1), (1,1), 'same', 'lrelu'], 'b2':[128, (3,3), (1,1), 'same', 'lrelu'],\n",
    "                         'b3':[128, (1,1), (1,1), 'same', 'lrelu'], 'b4':[256, (3,3), (1,1), 'same', 'lrelu']},\n",
    "               'block4':{'b1':[256, (1,1), (1,1), 'same', 'lrelu'], 'b2':[256, (3,3), (1,1), 'same', 'lrelu']}}\n",
    "\n",
    "inputs = Input(shape=(144, 144, 1), name=\"input_layer\")\n",
    "\n",
    "model = MNIST_YOLO(inpshape=(144, 144, 1),\n",
    "                   bocks_info=blocks_dict,\n",
    "                   grid_size=7, no_of_bnb=2,\n",
    "                   no_class=10,\n",
    "                   # inputs=inputs,\n",
    "                   # outputs=outputs\n",
    "                  )\n",
    "outputs = model(inputs)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.layers\n",
    "# print(model(tf.zeros([5,144,144,1])).shape)\n",
    "model.summary()\n",
    "# print(model.predict(tf.zeros([5,144,144,1])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49946de-1a6a-47e1-86c3-b7f59fe6efc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ResnetIdentityBlock(tf.keras.Model):\n",
    "#   def __init__(self, kernel_size, filters):\n",
    "#     super(ResnetIdentityBlock, self).__init__(name='')\n",
    "#     filters1, filters2, filters3 = filters\n",
    "\n",
    "#     self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "#     self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "#     self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "#     self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "#     self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "#     self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "#     self.A = Dense(32)\n",
    "#     self.flat = Flatten()\n",
    "\n",
    "#   def call(self, input_tensor, training=False):\n",
    "#     x = self.conv2a(input_tensor)\n",
    "#     x = self.bn2a(x, training=training)\n",
    "#     x = tf.nn.relu(x)\n",
    "\n",
    "#     x = self.conv2b(x)\n",
    "#     x = self.bn2b(x, training=training)\n",
    "#     x = tf.nn.relu(x)\n",
    "\n",
    "#     x = self.conv2c(x)\n",
    "#     x = self.bn2c(x, training=training)\n",
    "\n",
    "#     # x += input_tensor\n",
    "#     x = (x)\n",
    "#     x = self.A(x)\n",
    "#     return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "# block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "# _ = block(tf.zeros([1, 2, 3, 3]))\n",
    "# block.summary()\n",
    "# block.predict(tf.zeros([1,140,140,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141c48e-1bc8-44b0-ba55-b0a5f2a108d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers_list = list()\n",
    "\n",
    "nBlocks = 0\n",
    "nConvs = 0\n",
    "\n",
    "blocks_dict = {'block1':{'b1':[32, (7,7), (1,1), 'same', 'relu']},\n",
    "               'block2':{'b1':[64, (3,3), (1,1), 'same', 'relu']},\n",
    "               'block3':{'b1':[64, (1,1), (1,1), 'same', 'relu'], 'b2':[128, (3,3), (1,1), 'same', 'relu'],\n",
    "                         'b3':[128, (1,1), (1,1), 'same', 'relu'], 'b4':[256, (3,3), (1,1), 'same', 'relu']},\n",
    "               'block4':{'b1':[256, (1,1), (1,1), 'same', 'relu'], 'b2':[256, (3,3), (1,1), 'same', 'relu']}}\n",
    "\n",
    "def block_CN_gen(dictionary_info, layers_list):\n",
    "    '''generate a block of convolutions\n",
    "    dictionary_info : dict of list {kernel_list, stride_list, padding}'''\n",
    "    for key, (filt, ker, stride, pad, act) in dictionary_info.items():\n",
    "        # layer_name = 'block{}_conv{}'.format(nBlocks, nConvs)\n",
    "        layers_list.append(Conv2D(filt, kernel_size=ker, strides=stride, padding=pad,\n",
    "                           activation=act))#, name=layer_name))\n",
    "        # nConvs += 1\n",
    "    layers_list.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))#,\n",
    "                       # name='Mpooling_block{}'.format(nBlocks)))\n",
    "    # nBlocks += 1\n",
    "for key, val in blocks_dict.items():\n",
    "    block_CN_gen(val, layers_list)\n",
    "model = Sequential([Input(shape=(144, 144, 1), name=\"input_layer\"),\n",
    "                   ])\n",
    "for x in layers_list:\n",
    "    model.add(x)\n",
    "model.add(Conv2D(256, kernel_size=3, strides=1, activation='relu', name='conv2D_last'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(((10+2*5)*7**2)/4))\n",
    "        # self.drop =  Dropout(0.3)\n",
    "model.add(Dense(((10+2*5)*7**2), activation='sigmoid'))\n",
    "model.add(Yolo_Reshape())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01b1d631-946e-4626-9060-5863265e00de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 144, 144, 64)      3200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 72, 72, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 192)       110784    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 192)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 36, 36, 128)       24704     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 36, 36, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 36, 36, 256)       65792     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 36, 36, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 18, 18, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 18, 18, 256)       131328    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 9, 9, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               6554112   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 980)               502740    \n",
      "                                                                 \n",
      " yolo__reshape_2 (Yolo_Resha  (None, 7, 7, 20)         0         \n",
      " pe)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,767,764\n",
      "Trainable params: 14,767,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "\n",
    "# nb_boxes=1\n",
    "# grid_w=7\n",
    "# grid_h=7\n",
    "# cell_w=64\n",
    "# cell_h=64\n",
    "img_w=144\n",
    "img_h=144\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), input_shape =(img_h, img_w, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "# model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))\n",
    "# model.add(Conv2D(filters=512, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))#inventada\n",
    "\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "# model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))#inventada\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))#inventada\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "# model.add(Dense(1024))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7*7*20, activation='sigmoid'))\n",
    "model.add(Yolo_Reshape())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf0c48-ba37-4573-95ab-3964d133a25e",
   "metadata": {},
   "source": [
    "## 2.2 Creating a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa806093-09a7-44e9-9203-37b7dd1ac91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorA = np.arange(1*7*7*15).reshape([1,7,7,15])\n",
    "# tensorA = tf.Variable(tensorA)\n",
    "# tensorB = np.arange(1*7*7*15).reshape([1,7,7,15])*2\n",
    "# tensorB = tf.Variable(tensorB)\n",
    "# # tf.concat([tensorA,tensorA*2,tensorA*3],axis=-1)\n",
    "# # tf.math.reduce_sum(tf.subtract(tensorB,tensorA),axis=-1)\n",
    "# tensorB[:,:,:,0:1]<=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c821d-8b6e-4094-9344-691c8dfb16f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class yolo_loss_function(tf.keras.losses.Loss):\n",
    "    '''\n",
    "    This class is the loss function for yolo_v1, which is divided in\n",
    "    in 4 parts\n",
    "    '''\n",
    "\n",
    "    def __init__(self, grid_size, no_of_bnb, no_class):\n",
    "        '''Initialating que variables depending on the accuracy of the NN'''\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.no_of_bnb = no_of_bnb\n",
    "        self.no_class = no_class\n",
    "        self.coord_param = 5.0\n",
    "        self.noobj_param = 0.5\n",
    "\n",
    "    def get_xy_wh_global(self, xy_local_tensor):\n",
    "        '''\n",
    "        Translating the local coordintes (grid) xy,\n",
    "        in global coordinates (picture) -> [0,1]\n",
    "        Args:\n",
    "        -xy_local tensor: tf tensor with local coorinates\n",
    "        return: xy tensor in global coodinates\n",
    "        '''\n",
    "        # shape = xy_local_tensor.shape[0]\n",
    "        x_global = tf.range(0, self.grid_size, delta=1, dtype=tf.float32, name='range')\n",
    "        x_global = tf.expand_dims(x_global, axis=0)\n",
    "        x_global = tf.expand_dims(x_global, axis=0)\n",
    "        x_global = tf.tile(x_global, [1, self.grid_size, 1])\n",
    "        x_global = tf.expand_dims(x_global, axis=-1)\n",
    "        y_global = tf.range(0, self.grid_size, delta=1, dtype=tf.float32, name='range')\n",
    "        y_global = tf.expand_dims(y_global, axis=0)\n",
    "        y_global = tf.expand_dims(y_global, axis=-1)\n",
    "        y_global = tf.tile(y_global, [1, 1, self.grid_size])\n",
    "        y_global = tf.expand_dims(y_global, axis=-1)\n",
    "        xy_global = tf.concat([x_global, y_global], axis=-1)\n",
    "        # print(xy_global.shape)\n",
    "        # xy_global = tf.tile(xy_global, [shape, 1, 1, 1])\n",
    "        xy_global = tf.add(xy_local_tensor, xy_global)\n",
    "        xy_global = tf.multiply(xy_global, 1/self.grid_size)\n",
    "        return xy_global\n",
    "\n",
    "    def xy_minmax(self, xy_tensor, wh_tensor):\n",
    "        '''\n",
    "        Transform bnb xy centered coordinates in\n",
    "        xy_minmax corners of the bnb\n",
    "        Args:\n",
    "        -xy_tensor: tf tensor with xy centered coordinates\n",
    "        -wh_tensor: tf tensor wiht wh\n",
    "        return xy_min, xy_max corners of the bnb'''\n",
    "        xy_min = xy_tensor - wh_tensor/2\n",
    "        xy_max = xy_tensor + wh_tensor/2\n",
    "        return xy_min, xy_max\n",
    "\n",
    "    def iou(self, xy_pred_mins, xy_pred_maxes, xy_true_mins, xy_true_maxes):\n",
    "        intersection_min = tf.math.maximum(xy_pred_mins, xy_true_mins)     # ?*7*7*2\n",
    "        intersection_max = tf.math.minimum(xy_pred_maxes, xy_true_maxes)   # ?*7*7*2\n",
    "        intersection = tf.math.minimum(tf.subtract(intersection_max, intersection_min), 0.0)  # ?*7*7*2\n",
    "        intersection = tf.multiply(intersection[:, :, :, 0], intersection[:, :, :, 1])              # ?*7*7\n",
    "\n",
    "        pred_area = tf.subtract(xy_pred_maxes, xy_pred_mins)                   # ?*7*7*2\n",
    "        pred_area = tf.multiply(pred_area[:, :, :, 0], pred_area[:, :, :, 0])  # ?*7*7\n",
    "        true_area = tf.subtract(xy_true_maxes, xy_true_mins)                   # ?*7*7*2\n",
    "        true_area = tf.multiply(true_area[:, :, :, 0], true_area[:, :, :, 1])  # ?*7*7\n",
    "\n",
    "        union_areas = pred_area + true_area - intersection  # ?*7*7\n",
    "        iou_scores = intersection / union_areas             # ?*7*7\n",
    "        iou_scores = tf.expand_dims(iou_scores, axis=-1)    # ?*7*7*1\n",
    "\n",
    "        return iou_scores\n",
    "\n",
    "    def choose_max(self, list_of_tensor):\n",
    "        for t, tensor in enumerate(list_of_tensor):\n",
    "            tensor_shape = list_of_tensor[0].shape[0]\n",
    "            if t == 0:\n",
    "                init_tensor = tensor           # ?*7*7*1\n",
    "                # this create a mask of 0s with shape tensor\n",
    "                # tf.zeros cant be use\n",
    "                mask = tf.where(tensor!=-10, 0.0, -1.0)  # ?*7*7*1\n",
    "            else:\n",
    "                init_tensor = tf.math.maximum(init_tensor, tensor)  # ?*7*7*1\n",
    "                mask = tf.where(init_tensor == tensor, tf.cast(t, tf.float32), mask)     # ?*7*7*1\n",
    "        # Create a tensor mask for predict\n",
    "        # tensor_shape.append(self.no_of_bnb)\n",
    "        for i in range(len(list_of_tensor)):\n",
    "            if i == 0:\n",
    "                mask_pred = tf.where(mask == i, 1.0, 0.0)\n",
    "            else:\n",
    "                mask_pred = tf.concat([mask_pred,\n",
    "                                       tf.where(mask == i, 1.0, 0.0)], axis=-1)\n",
    "\n",
    "        # Create a tensor mask for bnb\n",
    "        for i in range(len(list_of_tensor)):\n",
    "            if i == 0:\n",
    "                mask_bnb = tf.tile(tf.where(mask == i, 1.0, 0.0), [1, 1, 1, 4])\n",
    "            else:\n",
    "                mask_bnb = tf.concat([mask_bnb,\n",
    "                                      tf.tile(tf.where(mask == i, 1.0, 0.0),\n",
    "                                              [1, 1, 1, 4])], axis=-1)\n",
    "\n",
    "        return mask_pred, mask_bnb\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # True labels\n",
    "        label_class = y_true[:, :, :, :self.no_class]               # ?*7*7*10\n",
    "        label_box = y_true[:, :, :, self.no_class:self.no_class+4]  # ?*7*7*4\n",
    "        response_mask = y_true[:, :, :, -1:]                        # ?*7*7*1\n",
    "        # Predicted labels\n",
    "        pred_class = y_pred[:, :, :, :self.no_class]                              # ?*7*7*10\n",
    "        pred_trust = y_pred[:, :, :, self.no_class:self.no_class+self.no_of_bnb]  # ?*7*7*2\n",
    "        pred_box = y_pred[:, :, :, self.no_class+self.no_of_bnb:]                 # ?*7*7*8\n",
    "        # Creating a xy label for the corners of the bnb\n",
    "        xy_label_box = self.get_xy_wh_global(label_box[:, :, :, :2])              # ?*7*7*2\n",
    "        wh_label_box = label_box[:, :, :, 2:]                                     # ?*7*7*2\n",
    "        xy_min_label_box, xy_max_label_box = self.xy_minmax(xy_label_box, wh_label_box)\n",
    "        xy_pred_box = [self.get_xy_wh_global(pred_box[:, :, :, 4*x:4*x+2]) for x in range(self.no_of_bnb)]  # [?*7*7*2]\n",
    "        wh_pred_box = [pred_box[:, :, :, 4*x+2:4*x+4] for x in range(self.no_of_bnb)]                       # [?*7*7*2]\n",
    "        xy_minmax_pred_box = [list(self.xy_minmax(xy, wh)) for xy, wh in zip(xy_pred_box,wh_pred_box)]      # [[?*7*7*2, ?*7*7*2]]]]\n",
    "\n",
    "        iou_scores = [self.iou(xy_minmax_pred_box[x][0],\n",
    "                               xy_minmax_pred_box[x][1],\n",
    "                               xy_min_label_box,\n",
    "                               xy_max_label_box) for x in range(self.no_of_bnb)]\n",
    "        mask_pred, mask_bnb = self.choose_max(iou_scores)  # ?*7*7*2, ?*7*7*8\n",
    "        # Calculating the losses of the centers of the objects\n",
    "        # coord * SUM(Iobj_i-j((x - xhat)² + (y - yhat)²))\n",
    "        loss_xy = tf.subtract(tf.concat(xy_pred_box, axis=-1),\n",
    "                              tf.tile(xy_label_box, [1, 1, 1, self.no_of_bnb]))\n",
    "        loss_xy = tf.tile(mask_pred, [1, 1, 1, self.no_of_bnb])*loss_xy**2\n",
    "        loss_xy = tf.tile(response_mask, [1, 1, 1, self.no_of_bnb*2])*loss_xy\n",
    "        loss_xy = tf.reduce_sum(loss_xy)\n",
    "        # print(loss_xy)\n",
    "        # Calculating the losses of the weith and heigth of the bb\n",
    "        # coord * SUM(Iobj_i-j((w^0.5 - what^0.5)² + (h^0.5 - hhat^0.5)²))\n",
    "        loss_wh = tf.subtract(tf.math.sqrt(tf.concat(wh_pred_box, axis=-1)),\n",
    "                              tf.math.sqrt(tf.tile(wh_label_box, [1, 1, 1, self.no_of_bnb])))\n",
    "        # print(loss_wh)\n",
    "        loss_wh = tf.tile(mask_pred, [1, 1, 1, self.no_of_bnb])*loss_wh**2\n",
    "        loss_wh = tf.tile(response_mask, [1, 1, 1, self.no_of_bnb*2])*loss_wh\n",
    "        loss_wh = tf.reduce_sum(loss_wh)\n",
    "        # print(loss_wh)\n",
    "        # Calculating the losses of the confidence in the predictions\n",
    "        # SUM(Iobj_i-j((C - Chat)²)) + no_objt_param*SUM(Inoobj_i-j(h^0.5 - hhat^0.5)²)\n",
    "        # Inoobj_i-j es de oposite of Iobj_i-j\n",
    "        loss_conf = pred_trust-tf.tile(response_mask,\n",
    "                                       [1, 1, 1, self.no_of_bnb])\n",
    "        loss_conf_noobj = tf.tile(1-response_mask,\n",
    "                                     [1, 1, 1, self.no_of_bnb])*mask_pred*loss_conf**2\n",
    "        loss_conf = tf.tile(response_mask,\n",
    "                            [1, 1, 1, self.no_of_bnb])*mask_pred*loss_conf**2\n",
    "        loss_conf_noobj = tf.reduce_sum(loss_conf_noobj)\n",
    "        loss_conf = tf.reduce_sum(loss_conf)\n",
    "        # print(mask_pred)\n",
    "        # print(tf.tile(response_mask,[1, 1, 1, self.no_of_bnb])*mask_pred)\n",
    "        # print(tf.tile(response_mask,[1, 1, 1, self.no_of_bnb]))\n",
    "        # print(loss_conf)\n",
    "        # print(loss_conf_noobj)\n",
    "        # Calculating the losses of the classes\n",
    "        # SUM_everycell(SUM_classes((C - Chat)²))\n",
    "        classes_loss = tf.subtract(label_class, pred_class)\n",
    "        classes_loss = tf.multiply(response_mask, tf.math.reduce_sum(classes_loss**2, axis=-1, keepdims=True))\n",
    "        classes_loss = tf.math.reduce_sum(classes_loss)\n",
    "        # print(classes_loss)\n",
    "        # Calculating the total loss\n",
    "        total_loss = self.coord_param*(loss_xy + loss_wh) + loss_conf + self.noobj_param*loss_conf_noobj + classes_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b385f-aea4-4b8c-a1cd-7084ee568281",
   "metadata": {},
   "source": [
    "## 2.3 Custom learning rate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a17831-bf68-441d-883c-f7c82489a2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    # (0, 0.01),\n",
    "    # (75, 0.001),\n",
    "    # (105, 0.0001),\n",
    "    (0, 0.05),\n",
    "    (40, 0.01),\n",
    "    (80, 0.005),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2adb5-37d8-4a84-bd39-15c258106080",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <b> 3. CREATING THE DATASET TO TRAIN THE NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ee19e-d2f3-47d7-8ff8-43aaea9624b2",
   "metadata": {},
   "source": [
    "## 3.1 Pascal Voc reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddcc02ea-4862-4316-b0fa-a2cf17c99b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the mapping function for the dataset. The dataset read the\n",
    "# files of the directory and generate a list of paths, it will return\n",
    "# a tupple with (image, data_info)\n",
    "\n",
    "def pascal_voc_to_dict(filepath):\n",
    "    \"\"\"Function to get all the objects from the annotation XML file.\"\"\"\n",
    "    # The next 3 lines must be modify if the dataset or nn change:\n",
    "    grid_size = 7\n",
    "    no_class = 10\n",
    "    no_of_bnb = 2\n",
    "\n",
    "    with tf.io.gfile.GFile(filepath.numpy().decode(), \"r\") as f:\n",
    "        # print(f.read())\n",
    "        root = ET.parse(f).getroot()\n",
    "\n",
    "        size = root.find(\"size\")\n",
    "        image_w = float(size.find(\"width\").text)\n",
    "        image_h = float(size.find(\"height\").text)\n",
    "        filePath = str(root.find(\"path\").text)\n",
    "        img = tf.io.read_file(filePath)\n",
    "        img = tf.io.decode_jpeg(img, channels=1).numpy()/255.0\n",
    "        # Setting up the output tensor\n",
    "        y_true_tensor = np.zeros([grid_size, grid_size, no_class+5])\n",
    "\n",
    "        for obj in root.findall(\"object\"):\n",
    "            # Get object's label name.\n",
    "            label = int(obj.find(\"name\").text)\n",
    "            # Get objects' pose name.\n",
    "            # pose = obj.find(\"pose\").text.lower()\n",
    "            # is_truncated = obj.find(\"truncated\").text == \"1\"\n",
    "            # is_difficult = obj.find(\"difficult\").text == \"1\"\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            xmax = float(bndbox.find(\"xmax\").text)\n",
    "            xmin = float(bndbox.find(\"xmin\").text)\n",
    "            ymax = float(bndbox.find(\"ymax\").text)\n",
    "            ymin = float(bndbox.find(\"ymin\").text)\n",
    "            # Calculating the middle point of the image and\n",
    "            # (h,w) adimensional -> [0,1]\n",
    "            x = (xmin + xmax) / 2 / image_w\n",
    "            y = (ymin + ymax) / 2 / image_h\n",
    "            w = (xmax - xmin) / image_w\n",
    "            h = (ymax - ymin) / image_h\n",
    "            # Calculate the position of the center inside\n",
    "            # of the grid (i,j) -> (x,y)\n",
    "            loc = [grid_size * x, grid_size * y]\n",
    "            loc_i = int(loc[1])\n",
    "            loc_j = int(loc[0])\n",
    "            y = loc[1] - loc_i\n",
    "            x = loc[0] - loc_j\n",
    "\n",
    "            if y_true_tensor[loc_i, loc_j, no_class+4] == 0:\n",
    "                y_true_tensor[loc_i, loc_j, label] = 1\n",
    "                y_true_tensor[loc_i, loc_j, no_class:no_class+4] = [x, y, w, h]\n",
    "                y_true_tensor[loc_i, loc_j, no_class+4] = 1  # response\n",
    "\n",
    "        return img, y_true_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834fd1a1-eb97-4f62-babf-888fd1e55d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "BATCH = 20\n",
    "PREFETCH = 20\n",
    "SHUFFLE = 200\n",
    "\n",
    "train = tf.data.Dataset.list_files(paths['train_data']+'/*.xml')\n",
    "train = train.map(lambda x: tf.py_function(pascal_voc_to_dict,inp=[x],Tout=[tf.float32, tf.float32]))\n",
    "train =train.batch(BATCH).shuffle(PREFETCH).prefetch(SHUFFLE)\n",
    "# list(train)[0][1]\n",
    "test = tf.data.Dataset.list_files(paths['test_data']+'/*.xml')\n",
    "test = test.map(lambda x: tf.py_function(pascal_voc_to_dict,inp=[x],Tout=[tf.float32, tf.float32]))\n",
    "test =test.batch(10)\n",
    "# x = list()\n",
    "# y = list()\n",
    "# for ele  in train.as_numpy_iterator():\n",
    "#     x.append(ele[0])\n",
    "#     y.append(ele[1])\n",
    "# len(x)\n",
    "# x=np.array(x)\n",
    "# # x = x.reshape(x.shape[1:])\n",
    "# y[0].shape\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c79318-7ac9-43c9-a31e-85e213fbc0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = data.batch(2)\n",
    "A = train.as_numpy_iterator().next()\n",
    "# model(A)\n",
    "A[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba1d3d5-6b2d-4541-870d-f1de4a95e015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_loss = yolo_loss_function(7, 2, 10)\n",
    "model.compile(loss=yolo_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f56e84-e49d-4bc0-a3ee-562d27cf8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.reshape(tf.range(0,2*7*7*15,1,dtype=tf.float32),[2,7,7,15])#/(2*7*7*15-1)\n",
    "# B = tf.tile(A,[1,1,1,2])\n",
    "# B[:,:,:,:15]-B[:,:,:,15:]\n",
    "# print(A)\n",
    "B = tf.concat([A[:,:,:,0:10],A[:,:,:,-1:],A[:,:,:,-1:],A[:,:,:,10:14],A[:,:,:,10:14]],\n",
    "              axis=-1)+0.1\n",
    "# # print(B)\n",
    "# yolo_loss(A,B)\n",
    "yolo_loss.call(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51e602-a7de-4243-85e1-b54b3f92aacf",
   "metadata": {},
   "source": [
    "# <b> 4. TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed3a72c-6ba1-4b2b-9f1a-886e58da9611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.0500.\n",
      "Epoch 1/135\n",
      "141/141 [==============================] - 184s 1s/step - loss: 299.8214 - val_loss: 127.3603\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0500.\n",
      "Epoch 2/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 246.8347 - val_loss: 121.7734\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0500.\n",
      "Epoch 3/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 244.3688 - val_loss: 120.7730\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0500.\n",
      "Epoch 4/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 254.3072 - val_loss: 126.0070\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0500.\n",
      "Epoch 5/135\n",
      "141/141 [==============================] - 170s 1s/step - loss: 267.1201 - val_loss: 129.4836\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0500.\n",
      "Epoch 6/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 257.1408 - val_loss: 128.0795\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0500.\n",
      "Epoch 7/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 258.3087 - val_loss: 126.8784\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0500.\n",
      "Epoch 8/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 257.6276 - val_loss: 125.9890\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0500.\n",
      "Epoch 9/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 256.3369 - val_loss: 125.5123\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0500.\n",
      "Epoch 10/135\n",
      "141/141 [==============================] - 170s 1s/step - loss: 253.0104 - val_loss: 123.2535\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0500.\n",
      "Epoch 11/135\n",
      "141/141 [==============================] - 170s 1s/step - loss: 254.3318 - val_loss: 125.9861\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0500.\n",
      "Epoch 12/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 246.3854 - val_loss: 121.3214\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0500.\n",
      "Epoch 13/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 245.7888 - val_loss: 121.1399\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0500.\n",
      "Epoch 14/135\n",
      "141/141 [==============================] - 171s 1s/step - loss: 245.7173 - val_loss: 121.0827\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0500.\n",
      "Epoch 15/135\n",
      "112/141 [======================>.......] - ETA: 32s - loss: 246.7643"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      4\u001b[0m mcp_save \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# steps_per_epoch=2,  # int(len(X_train) // batch_size),\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m135\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# verbose=1,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# workers=4,\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# # validation_steps = int(len(X_val) // batch_size),\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mCustomLearningRateScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;66;43;03m#,\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m#mcp_save\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m#]\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\Javi\\Python\\MNIST_OD\\MNIST_OD\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "model.fit(x=train,\n",
    "          # steps_per_epoch=2,  # int(len(X_train) // batch_size),\n",
    "          epochs=135,\n",
    "          # verbose=1,\n",
    "          # workers=4,\n",
    "          validation_data = test,\n",
    "          # # validation_steps = int(len(X_val) // batch_size),\n",
    "          callbacks=[CustomLearningRateScheduler(lr_schedule)]#,\n",
    "                     #mcp_save\n",
    "                    #]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "948f8dba-e759-46a4-a853-72c3c177f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def xywh2minmax(xy, wh):\n",
    "    xy_min = xy - wh / 2\n",
    "    xy_max = xy + wh / 2\n",
    "\n",
    "    return xy_min, xy_max\n",
    "\n",
    "\n",
    "def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    pred_wh = pred_maxes - pred_mins\n",
    "    true_wh = true_maxes - true_mins\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas\n",
    "\n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "def yolo_head(feats):\n",
    "    # Dynamic implementation of conv dims for fully convolutional model.\n",
    "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
    "    # In YOLO the height index is the inner most iteration.\n",
    "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
    "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
    "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
    "\n",
    "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
    "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
    "    conv_width_index = K.tile(\n",
    "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
    "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
    "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
    "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
    "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
    "\n",
    "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
    "\n",
    "    box_xy = (feats[..., :2] + conv_index) / conv_dims\n",
    "    box_wh = feats[..., 2:4]\n",
    "\n",
    "    return box_xy, box_wh\n",
    "\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    label_class = y_true[..., :10]  # ? * 7 * 7 * 20\n",
    "    label_box = y_true[..., 10:14]  # ? * 7 * 7 * 4\n",
    "    response_mask = y_true[..., 14]  # ? * 7 * 7\n",
    "    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n",
    "\n",
    "    predict_class = y_pred[..., :10]  # ? * 7 * 7 * 20\n",
    "    predict_trust = y_pred[..., 10:12]  # ? * 7 * 7 * 2\n",
    "    predict_box = y_pred[..., 12:]  # ? * 7 * 7 * 8\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
    "\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
    "\n",
    "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n",
    "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n",
    "\n",
    "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n",
    "    # print((1 - box_mask * response_mask).shape)\n",
    "    # print((K.square(predict_trust)).shape)\n",
    "\n",
    "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
    "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
    "    confidence_loss = no_object_loss + object_loss\n",
    "    confidence_loss = K.sum(confidence_loss)\n",
    "\n",
    "    class_loss = response_mask * K.square(label_class - predict_class)\n",
    "    class_loss = K.sum(class_loss)\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "\n",
    "    box_mask = K.expand_dims(box_mask)\n",
    "    response_mask = K.expand_dims(response_mask)\n",
    "\n",
    "    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
    "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
    "    box_loss = K.sum(box_loss)\n",
    "    # print(box_loss)\n",
    "    # print(class_loss)\n",
    "    # print(confidence_loss)\n",
    "\n",
    "    loss = confidence_loss + class_loss + box_loss\n",
    "    # loss.shape\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdc72e-2cdf-4e31-baac-1a5b2ae253f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.range(0, 4*4*3, delta=1, dtype=tf.float32)\n",
    "A = tf.reshape(A,[4,4,3])\n",
    "B = tf.ones([4,4,1], dtype=tf.float32)+2\n",
    "tf.add(A,B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d99478-19d0-453e-a64a-3f0498da5d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNIST_OD",
   "language": "python",
   "name": "mnist_od"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
